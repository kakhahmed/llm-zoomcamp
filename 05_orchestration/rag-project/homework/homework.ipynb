{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'data_loader' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_loader\n",
    "if 'test' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import test\n",
    "\n",
    "\n",
    "@data_loader\n",
    "def load_data(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Template code for loading data from any source.\n",
    "\n",
    "    Returns:\n",
    "        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n",
    "    \"\"\"\n",
    "    # Specify your data loading logic here\n",
    "    import io\n",
    "\n",
    "    import requests\n",
    "    import docx\n",
    "\n",
    "    def clean_line(line):\n",
    "        line = line.strip()\n",
    "        line = line.strip('\\uFEFF')\n",
    "        return line\n",
    "\n",
    "    def read_faq(file_id):\n",
    "        url = f'https://docs.google.com/document/d/{file_id}/export?format=docx'\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with io.BytesIO(response.content) as f_in:\n",
    "            doc = docx.Document(f_in)\n",
    "\n",
    "        questions = []\n",
    "\n",
    "        question_heading_style = 'heading 2'\n",
    "        section_heading_style = 'heading 1'\n",
    "        \n",
    "        heading_id = ''\n",
    "        section_title = ''\n",
    "        question_title = ''\n",
    "        answer_text_so_far = ''\n",
    "        \n",
    "        for p in doc.paragraphs:\n",
    "            style = p.style.name.lower()\n",
    "            p_text = clean_line(p.text)\n",
    "        \n",
    "            if len(p_text) == 0:\n",
    "                continue\n",
    "        \n",
    "            if style == section_heading_style:\n",
    "                section_title = p_text\n",
    "                continue\n",
    "        \n",
    "            if style == question_heading_style:\n",
    "                answer_text_so_far = answer_text_so_far.strip()\n",
    "                if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "                    questions.append({\n",
    "                        'text': answer_text_so_far,\n",
    "                        'section': section_title,\n",
    "                        'question': question_title,\n",
    "                    })\n",
    "                    answer_text_so_far = ''\n",
    "        \n",
    "                question_title = p_text\n",
    "                continue\n",
    "            \n",
    "            answer_text_so_far += '\\n' + p_text\n",
    "        \n",
    "        answer_text_so_far = answer_text_so_far.strip()\n",
    "        if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "            questions.append({\n",
    "                'text': answer_text_so_far,\n",
    "                'section': section_title,\n",
    "                'question': question_title,\n",
    "            })\n",
    "\n",
    "        return questions\n",
    "\n",
    "    faq_documents = {\n",
    "        'llm-zoomcamp': '1T3MdwUvqCL3jrh3d3VCXQ8xE0UqRzI3bfgpfBq3ZWG0',\n",
    "    }\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for course, file_id in faq_documents.items():\n",
    "        print(course)\n",
    "        course_documents = read_faq(file_id)\n",
    "        documents.append({'course': course, 'documents': course_documents})\n",
    "    print(\"documents\", len(documents))\n",
    "    total_questions = sum(len(course['documents']) for course in documents)\n",
    "    print(f\"Total FAQ entries processed: {total_questions}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "@test\n",
    "def test_output(output, *args) -> None:\n",
    "    \"\"\"\n",
    "    Template code for testing the output of the block.\n",
    "    \"\"\"\n",
    "    assert output is not None, 'The output is undefined'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List, Union\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.exceptions import ConnectionError\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "from mage_ai.data_preparation.variable_manager import set_global_variable\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@data_exporter\n",
    "def elasticsearch(documents, *args, **kwargs):\n",
    "    connection_string = kwargs.get('connection_string', 'http://elasticsearch:9200')\n",
    "    index_name_prefix = kwargs.get('index_name', 'documents')\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    index_name = f\"{index_name_prefix}_{current_time}\"\n",
    "    print(\"index_name\", index_name)\n",
    "    set_global_variable(\"ragic\", 'index_name', index_name)\n",
    "\n",
    "    logger.info(f\"index name: {index_name}\")\n",
    "    number_of_shards = kwargs.get('number_of_shards', 1)\n",
    "    number_of_replicas = kwargs.get('number_of_replicas', 0)\n",
    "    vector_column_name = kwargs.get('vector_column_name', 'embedding')\n",
    "    # Ensure dimensions is a positive integer\n",
    "    #dimensions = kwargs.get('dimensions', 0)\n",
    "    #if not isinstance(dimensions, int) or dimensions <= 0:\n",
    "    #    print(\"dimensions\", dimensions)\n",
    "    #    raise ValueError(\"'dimensions' must be a positive integer\")\n",
    "\n",
    "    es_client = Elasticsearch([connection_string])\n",
    "    logger.info(f'Connecting to Elasticsearch at {connection_string}')\n",
    "\n",
    "    if not es_client.ping():\n",
    "        raise ConnectionError(\"Failed to connect to Elasticsearch\")\n",
    "\n",
    "    index_settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": number_of_shards,\n",
    "            \"number_of_replicas\": number_of_replicas\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"text\": {\"type\": \"text\"},\n",
    "                \"section\": {\"type\": \"text\"},\n",
    "                \"question\": {\"type\": \"text\"},\n",
    "                \"course\": {\"type\": \"keyword\"},\n",
    "                \"document_id\": {\"type\": \"keyword\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if es_client.indices.exists(index=index_name):\n",
    "        es_client.indices.delete(index=index_name)\n",
    "        logger.info(f'Index {index_name} deleted')\n",
    "\n",
    "    es_client.indices.create(index=index_name, body=index_settings)\n",
    "    logger.info('Index created with properties:')\n",
    "    logger.info(json.dumps(index_settings, indent=2))\n",
    "    #logger.info(f'Embedding dimensions: {dimensions}')\n",
    "\n",
    "    count = len(documents)\n",
    "    logger.info(f'Indexing {count} documents to Elasticsearch index {index_name}')\n",
    "    for idx, document in enumerate(documents):\n",
    "        if idx % 100 == 0:\n",
    "            logger.info(f'{idx + 1}/{count}')\n",
    "\n",
    "        #if isinstance(document.get('vector_column_name'), np.ndarray):\n",
    "        #     document['vector_column_name'] = document['vector_column_name'].tolist()\n",
    "        # Ensure the embedding is the correct length\n",
    "        #if len(document.get('vector_column_name', [])) != dimensions:\n",
    "        #    print(f\"Document {idx} has incorrect embedding dimensions. Expected {dimensions}, \"\n",
    "        #         f\"got {len(document.get('vector_column_name', []))}. Skipping.\")\n",
    "        #    continue\n",
    "        \n",
    "        es_client.index(index=index_name, document=document)\n",
    "    print(\"last document\", document)\n",
    "\n",
    "    logger.info(\"Indexing completed\")\n",
    "    return [[d.get('vector_colum') for d in documents[:10]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from typing import Dict, List, Union\n",
    "\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "if 'data_loader' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_loader\n",
    "if 'test' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import test\n",
    "\n",
    "\n",
    "@data_loader\n",
    "def search(*args, **kwargs) -> List[Dict]:\n",
    "    connection_string = kwargs.get('connection_string', 'http://localhost:9200')\n",
    "    index_name = \"documents_20240820_203833\"\n",
    "    source = kwargs.get('source', \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\")\n",
    "    top_k = kwargs.get('top_k', 5)\n",
    "    chunk_column = kwargs.get('chunk_column', 'content')\n",
    "\n",
    "    #if isinstance(query_embedding, np.ndarray):\n",
    "    #    query_embedding = query_embedding.tolist()\n",
    "\n",
    "    query = \"When is the next cohort?\"\n",
    "    if len(args):\n",
    "        query = args[0]\n",
    "    if not query:\n",
    "        raise ValueError(\"A query must be provided\")\n",
    "\n",
    "    es_client = Elasticsearch(connection_string)\n",
    "    try:\n",
    "        def elastic_search(query):\n",
    "            search_query = {\n",
    "                \"query\": {\n",
    "                    \"bool\": {\n",
    "                        \"must\": {\n",
    "                            \"multi_match\": {\n",
    "                                \"query\": query,\n",
    "                                \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                                \"type\": \"best_fields\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"filter\": {\n",
    "                            \"term\": {\n",
    "                                \"course\": \"llm-zoomcamp\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            response = es_client.search(index=index_name, body=search_query)\n",
    "            \n",
    "            result_docs = []\n",
    "            \n",
    "            for hit in response['hits']['hits']:\n",
    "                result_docs.append(hit['_source'])\n",
    "            \n",
    "            return result_docs\n",
    "\n",
    "        print(\"Sending search query:\", query)\n",
    "\n",
    "\n",
    "        response = elastic_search(query)\n",
    "        print(\"Raw response from Elasticsearch:\", response)\n",
    "\n",
    "        return [hit['_source'][chunk_column] for hit in response['hits']['hits']]\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "@test\n",
    "def test_output(output, *args) -> None:\n",
    "    \"\"\"\n",
    "    Template code for testing the output of the block.\n",
    "    \"\"\"\n",
    "    assert output is not None, 'The output is undefined'"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
